## Описание программы

Этот код реализует простую нейронную сеть для классификации данных с использованием градиентного спуска. 

## Объяснение, что делают функции и зачем они нужны.

relu(t): Это функция активации ReLU (Rectified Linear Unit), которая принимает входной тензор t и возвращает тензор,
в котором все отрицательные значения заменены на нули. Функция ReLU используется для добавления нелинейности в нейронные сети.

softmax(t): Это функция softmax, которая принимает входной тензор t и возвращает вероятностное распределение по классам.
Она используется для преобразования вывода нейронной сети в вероятности классов.

softmax_batch(t): Это версия функции softmax, которая работает с пакетами данных, применяя softmax к каждому элементу пакета.

sparse_cross_entropy(z, y): Это функция для вычисления кросс-энтропии между предсказанными вероятностями z и истинными метками y.
Она используется как функция потерь для оценки качества модели.

sparse_cross_entropy_batch(z, y): Это версия функции кросс-энтропии, которая работает с пакетами данных.

to_full(y, num_classes) и to_full_batch(y, num_classes): Эти функции конвертируют целочисленные метки классов
в one-hot представление, где каждая метка представлена вектором длиной num_classes, содержащим нули везде,
кроме индекса метки класса.

relu_deriv(t): Это производная функции ReLU, которая используется при обратном распространении ошибки.

После определения этих функций, код создает нейронную сеть с двумя скрытыми слоями. Он использует набор данных Iris для обучения модели. Затем запускается процесс обучения с использованием градиентного спуска. Наконец, модель оценивается на точность классификации, и результат отображается на экране. Также строится график функции потерь во время обучения.

## Запустить тренировку
Примечание: веса не сохраняются

``
python train.py
```

## Выполнить логический вывод
Примечание: веса жестко заданы(определяет вид ирисов по значениям для проверки задан вид (Virginica))

``
python inference.py
```